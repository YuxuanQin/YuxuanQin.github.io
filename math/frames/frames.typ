#import "template.typ": conf // self template
#import "@preview/commute:0.2.0": node, arr, commutative-diagram // commutative diagram package
#import "theorems.typ": * // thm package
#show: thmrules // I don't know, laugh
#show: doc => conf(
  lang: "en",
  title: [Notes on *Frames and Locales*],
  authors: (
    (name: "Qin Yuxuan"),
  ),
  abstract: [This is my notes on Picardo and Pultr's book _Frames and Locales_, which is a good monograph of point-free topology. This notes will, at least, as my intent, _not_ be something contain the full contents of the book, instead, it should serve as a helper to prevent me from fogetting everything. And I hope this notes can clarify as much misunderstoodings as posible.],
  doc
)
#show "Pos": $bold("Pos")$ // poset cat 
#show "Lac": $bold("Lac")$ // lattice cat
#show "Frm": $bold("Frm")$ // frame cat
#show "Loc": $bold("Loc")$ // locale cat

/*-------------- Theorem enviroment ----------------*/
#let thm = thmbox(
  "theorem",
  "Theorem",
  fill: rgb("#e8e8f8")
)
#let lemma = thmbox(
  "theorem",            // Lemmas use the same counter as Theorems
  "Lemma",
  fill: rgb("#efe6ff")
)
#let prop = thmbox(
  "theorem",
  "Proposition",
  fill: rgb("d3d3d3")
)
#let cor = thmbox(
  "corollary",
  "Corollary",
  base: "theorem",      // Corollaries are 'attached' to Theorems
  fill: rgb("#f8e8e8")
)

#let def = thmbox(
  "definition",         // Definitions use their own counter
  "Definition",
  fill: rgb("#e8f8e8")
)

#let exercise = thmbox(
  "exercise",
  "Exercise",
  stroke: rgb("#ffaaaa") + 1pt,
  base: none,           // Unattached: count globally
).with(numbering: "I")  // Use Roman numerals

// Examples and remarks are not numbered
#let eg = thmplain("example", "Example").with(numbering: none)
#let remark = thmplain(
  "remark",
  "Remark",
  inset: 0em
).with(numbering: none)

// Proofs are attached to theorems, although they are not numbered
#let proof = thmproof(
  "proof",
  "Proof",
  base: "theorem",
)
/*-------------- Theorem enviroment ----------------*/


= June 1, 2024
There are many preliminaries needed be established first: the category Pos which consists of all partial order sets#footnote([Ingore size issues, of course.]), the category Lac of lattices, Galois adjunctions, Heyting and Boolean algebras. The story is not long, but give yourself some time to digest it and, _appreciate_ it.

== The category Pos and Loc
The essence of a category is its morphisms, which respect some specific structures, so what maps should be, or _can_ be morphisms? Well, just look at the poset's structures -- order.

#def[Monotone functions][
  Those polite maps _respect_ the structures of posets, i.e., a map $f : A -> B$ is called _monotone_ if and only if

  $ u <= v => f (u) <= f (v) . $ 

  Of course, both $A$ and $B$ are posets.
]

There is nothing more interesting than its morphisms, in Pos. However, the funny fact is that any poset $P$ is itslef a category, so the objects in the category Pos are also categories! We use $bold("Pos")(P)$ or just $P$ to denote the category generated by a single poset $P$, as follows:

#def[View posets as categories][
  Given any poset $P$, it can be viewd as a category, where

  - Objects: elements in it;
  - Morphisms: define morphisms between two objects (hence elements) $f : a -> b$ if and only if $a <= b$;
  - Identity morphisms exist: for all $a$, $a <= a$ by the definition of partial order, which induce a morphism;
  - Composition rules: by the transivity of partial order.

  #remark[Further, a functor between two categories $"Pos"(X)$ and $"Pos"(Y)$ is exactly a monotone map $m$ between $X$ and $Y$, viewd as posets.
  
  - Preserve morphisms: if $f : x_1 -> x_2$ is a morphism in $"Pos"(X)$, then there exist a morphism $m(f) : m(x_1) -> m(x_2)$ in $"Pos"(Y)$ as well. And this is just a fancy way to describe the monotonity;
  - Preserve indentity: also trivial;
  - Preserve composition: by monotonity and transivity.

  ]
]<poset>

In addition, one can find that the _suprema_ of a poset $A$ is just the _coproduct_ (or _sum_) of the categry, and _infima_ is the _product_. And the bottom $bot$ is exactly the initial object (hence we sometimes use $0$ to denote it), while $top$ is the terminal object (hence $1$, sometimes). This observation is important, since we know left adjoint preserve coproduct#footnote([Actually, co-limit. Also the "prodcut" later is "limit"]) and right adjoint preverse product.

#align(center, commutative-diagram(
  node((1, 1), [$inf {a, b} = a and b$]),
  node((2, 0), [$a$]),
  node((2, 2), [$b$]),
  node((0, 1), [lower bound]),
  arr((1, 1), (2, 0), [], curve: -30deg),
  arr((1, 1), (2, 2), [], curve: 30deg),
  arr((0, 1), (2, 0), [], curve: -30deg),
  arr((0, 1), (2, 2), [], curve: 30deg),
  arr((0, 1), (1, 1), []),
))

Don't forget the morphism $a -> b$ is defined if and only if $a <= b$.

== Galois adjunctions
Galois adjunctions are just adjunctions between two posets $A$ and $B$, viewd as categories, that's all:


$ "Hom"_A (f(a), b) tilde.equiv "Hom"_B (a, g(b)). $

Unpack the definition of adjunctions, we immediately get the explicit version of the Galois adjunctions between two posets $A$ and $B$:

#def[Galois adjunctions][Two monotone functions $f : A -> B$ and $g : B -> A$ are said to be _Galois adjoint_ if for all $a in A$ and $b in B$, we have
$ f(a) <= b <==> a <= g(b). $
]

This is essencially a reformation of the abstract definition. Note that, if we are in the category setting, the left adjoint of $g$ is _the_ $f$, since each left adjoint is isomorphic to others, of course right adjoints share this note as well. 

#eg[The so-called "to-range" map $f : cal(P)(A) -> cal(P)(B)$ with its "inverse" $f^(-1)$ are adjoint to each other, since 
  $ f(U) subset V <==> U subset f^(-1)(V). $
]

Here is a interesting proposition, where $f$ and $g$ are more closed to each other -- they are on the same side:

#prop[
  Two morphisms $f : X -> Y$ and $g : Y -> X$ in Pos are adjoint to each other, if and only if for all $x in X$ and $y in Y$, there holds

  $ f g (y) <= y " and " " " x <= g f (x). $
]
#proof[(Abstract nonsense version). As noted in @poset, $f$ and $g$ can be viewed as _functors_ as well. Guess what? By category theory, two functors are adjoint if and only if there exists natural transformations "unit" $eta : id_X ==> g f$ and "counit" $epsilon : f g ==> id_Y$, in diagram, it says, for all $x in X$,
#align(center, commutative-diagram(
  node((0, 0), [$x$]),
  node((1, 0), [$x$]),
  node((0, 1), [$g f (x)$]),
  node((1, 1), [$g f(x)$]),
  arr((0, 0), (1, 0), [$id_x$], "nat", label-pos: right),
  arr((0, 0), (0, 1), [], label-pos: 0),
  arr((0, 0), (0, 1), [$epsilon_x$]),
  arr((1, 0), (1, 1), [$epsilon_x$], label-pos: right),
  arr((0, 1), (1, 1), [$g f(id_x)$], label-pos: left),
)) 
commutes. Actually the commutativity is a little bit too much, we only need the existence of $epsilon_x$, which indicate that $x <= g f (x)$.

Man, what can I say? Category theory is baesd.
]

#proof[(Not based version). Note that the proposition has two sides:

- ($==>$) Move the outside $f$ and $g$ in both inequalities and immediately we find it obvious.
- ($<==$) This is more interesting. Keep in mind that now, we want to prove 
$ f(a) <= b <==> a <= g(b), $
so by the introduction rule, we firstly obtain $f(a) <= b$ and goal is $a <= g(b)$. So by assumption,

$ a <= g f(a) = g(f(a)) <= g(b), $ 

which is we wanted.
]

#thm[Left Galois adjoint preserve suprema, while the right one preserve infima.]
#proof[Guess what? By *Category theory*, left adjoint preserve not only coproduct (hence suprema), but also co-limit, and the situation of right adjoint is similar, qed.]

In lattice theory, the converse statement is also true:

#thm[If a monotone maps $f : X -> Y$ preverse suprema, then it is a left adjoint.]<thm1-2-3>
The goal is to construct, in a explicit way, a map $g : Y -> X$ satisfies the definition. That is to arrange each $y in Y$ to some value in $X$. Let's first assume $g$ is indeed athe right adjoint of $f$, and find some properties it *must* satisfy which guide our constructtion.

We claim that $g(y)$ *must* be the suprema of all $x$ such that $f(x) <= y$, i.e., this equation must holds:

$ g(y) = sup {x : f(x) <= y}. $

There is no doubt $g(y)$ is a general upper bound of ${x : f(x) <= y}$, also note this set contain $g(y)$ itself as well, since $g(y) <= g(y)$ and then by the adjunction's property. Accordingly, our cliam is true.

So there is actually no other ways to define the behaviour of $g$, we are *forced* to arrange each $y in Y$ to $sup {x : f(x) <= y}$, no other choices. 

#proof[of @thm1-2-3][
  Define $g : Y -> X, y |-> sup {x : f(x) <= y}$. And we verify this is indeed the right adjoint of $f$:

  - $f(x) <= y$ implies $x <= g(y)$: this is trivial by the definition of $g(y)$.

  - $x <= g(y)$ implies $f(x) <= y$: Please think about the assumption that $f$ preverse arbitrary suprema, which hasn't been used yet. Apply $f$ to both side of the inequality $x <= g(y)$, and by the monotonity:

  $ f(x) <= f(g(y)) = f(sup{u : f(u) <= y}); $

  since $f$ preserve suprema, the last stuff equals to $sup{f(u) : f(u) <= y}$, which is aparently less than or equal to $y$.
]

== Lattices
Lattices are just better posets, we concern thoes fancy sets since the topology of a sapce $X$, or $Omega X$, the set of all its open sets is indeed a lattice. #footnote([Why don't you use paratheness, namely use $Omega(X)$? Lol, because the world is ruled by *Category theory*, $Omega$ is a _functor_!])

#def[Lattices][Lattices are just posets, which are close under _finite_ meet and join.

#remark[In the book, the authors differ the notion of lattices and _bounded_ lattices, while we *do not* -- lattices _are_ bounded lattices.]
]

The notion of lattices is necessary, we show that there exists poset which is not a lattice: equip the two points set ${*, dagger}$ with the partial order defined as exactly equal, i.e., $x subset.sq y$ if and only if $x = y$. This dummy set is indeed a poset, but neither ($* and dagger$) nor ($* or dagger$) exists.

The following definition is a rutine.

#def[_Complete_ Lattices][If a lattice admits arbitrary size of meet and join, then it is called _complete_ lattice.]

//TODO: clarify that meet is not generally intersection.
//As promised, for all topology space $X$, the collection of its open set, namely $Omega X$, is a lattice, but this is not obvious. Equip $Omega X$ whit inclusion order, that is, let $U <= V$ if and only if $U subset V$, and now define the join $U or V = U union V$,

#set quote(block: true)
== Distributive lattices
Why study distributive lattices? Well, since $Omega X$ is a lattice, we'd better, then, clasify some specific lattices and capture as much properties as posible, and a satisfying thing is that

#quote(attribution: [#link("https://ncatlab.org/nlab/show/distributive+lattice")])[- Any Boolean algebra, and even any Heyting algebra, is a distributive lattice.
- Every frame and every $sigma$-frame is a distributive lattice.]

#def[Distributive lattices][A lattice is _distributive_ if and only if this distributive law holds:

$ a and (b or c) = (a and b) or (a and c). $

#remark[
  An immediate observation is that this law implies the dual one:
  $ a or (b and c) = (a or b) and (a or c); $
  which can be proved straightforward: $(a or b) and (a or c) = ((a or b) and a) or ((a or b) and c)$, which by the subsititution of the former one's $a$ by $(a or b)$, and the right-hand-side can be computed, with the fact $(a or b) and a = a$, as $"rhs" = a or ((c and a) or (c and b)) = a or (c and b)$, where the last equality followed by that $or$ is asscociative. 
]
]

By the remark above, we distil this conclusion:

#align(center)[A lattice $L$ is distributive $<==>$ its opposite lattice $L^"op"$ is distributive.]

There are more symmetric laws, but, please consult to ncatlab.

== Filters and ideals
So you already known what is a 

#def[Filter][A _filter_ $F$ on a distributive lattice $L$ is a subset of $L$, such that

- $1 in F$; (the top element is big)
- $a in F "and" b in F ==> a and b in F$; (two big sets' intersection is also big)
- $a in F, "and" b >= a ==> b in F$ (if you are bigger than a big guy, then you are big too)

#remark[Sometimes we talk about _prime_ filters, those filters somehow act like prime numbers: a filter $F$ is called _prime_ if and only if 

$ a or b in F ==> a in F "or" b in F; $
compare with prime numbers:

$ m times n = p ==> m bar p "or" n bar p. $

]
]

The definition of _ideal_ is just a filter in the opposite lattice, or just modify the above definition to a "small guys" version.

The main result of this section is the so called "ultralfilters exists" theorem, stated as bellow:

#lemma[Let $J$ be an ideal and $F$ be a filter, which satisfy their intersection is empty. Then there exists a maximal _prime filter_ $overline(F)$ which respect to the property $overline(F)$ does not interesects with $J$.]
#proof[There are two points to prove:

- Maximum: when talk about maximum, one should always remember the Zorn lemma, which is exactly a proposition about maximal things. And, note that,  there is not too much restrictions, being _envy_ to construct what you want is welcome. (proof omitted, as exercise. Lol)
- Primeness: this is more chanllengable, give me some times to digest it. // TODO: finish this proof
]


== Pseudocomplement
This is the last section of today (I hope), fuh!

Since defining Heyting algebra needs the preparation of pseudocomplement, we first define this slightly abstract notion:

#def[Pseudocomplement][In a $and$-lattice, The pseudocomplement of a element $a in F$ is the _largest_ element $x$ such that

$ x and a = 0, $

where "largest" means for all $b in F$,

$ b and a = 0 <==> b <= x. $

We use the convenient notation $a^*$ to denote this (unique) pseudocomplement.

#remark[Of course, this pseudocomplement need not to exist. For instance:

#align(center)[
```
...         ...
  \         /
   4       3
    \     /
     2   1
      \ /
       0

```
]
]

]

== Heyting algebras
Heyting algebras captrue the notion of "semantics implication" in some sense -- those structures are equipped with a so-called _Heyting operation_ "$->$", which can be indentified with the logic implication. There is a interesting correspondence between implications and complements of elements: in (classic) logic, $0$ is the complement of $1$ and vice versa, with these equations:

$ (1 -> 0) = "the complement of" 1; $
$ (0 -> 0) = "the complement of" 0; $

by definition of implication.

A Heyting algebra extends this correspondence: change "complement" to "pseudocomplement" and add more elements other than $1$ and $0$, at the same time preserving the equations above, namely the pseudocomplement $a^*$ of $a in H$ is equal to $a -> 0$. And this Heyting operation can be extended, too, that is we can define more formulars than just ($square -> 0$), so, finally, here is the definition:

#def[Heyting algebras][A lattice is called a _Heyting algebra_ if it is equipped with a binary _Heyting operation_ "$->$" such that

$ c <= a -> b "if and only if" c and a <= b. $

#remark[
  - This law is indeed a Galois adjunction, where the left adjoint is the map ($square |-> a and square$) and the right one is ($square |-> (a -> square)$). 
  - This law ensures that the Heyting operation is indeed a extension of implication of classic logic, i.e., 

  $ a^* = a -> 0, $
  where $a^*$ is the pseudocomplement of $a$.
]
]
The equation in the definition is *like* the definition of Galois adjunction, but we need to justify that these maps are really _leagal morphisms_ -- monotone maps.  

- The left adjoint is monotone: this is obvious, since by the definition of infima: $ x <= y ==> u and x <= u and y. $
- The right guy: suppose $x <= y$ and fix $u$, we want to prove $ (u -> x) <= (u -> y)$, which is equavalent to, by definition, $u and (u -> x) <= y$. The trick is: $ u and (u -> x) <= x, $ since $u -> x = u -> x$.

There is a striking result, of the specific left adjoint ($square |-> (a and square)$):

#thm[Every Heyting algebra is distributive.]
#proof[
  A lattice is distributive if and only if
  $ a and (b or c) = (a and b) or (a and c). $
  And a left adjoint preverse suprema (the $or$ operation), apply this little lemma to our left adjoint, we immediately obtain the distributive law.
]

Thanks to Galois adjunction, we can obtain two more amazing theorems about Heyting algebra, which is of sattisfactory, in all sense:

#thm[Heyting operation is unique]
#proof[By the uniqueness of adjoint, and notice the left one ($square |-> (a and square)$) is fixed.]

#thm[A lattice admits a Heyting algebra if this arbitrary distributive law holds:
$ b and (or_(i in I) a_i) = or_(i in I)(b and a_i). $
]
#proof[By @thm1-2-3, plus the observation of monotonity of the map ($square |-> (a and square)$), we obtain a right adjoint of this map, but don't forget the Heyting operation _is_ a right adjoint of it and, adjointy _is_ unique. We are done.]

Some warm notices:
- All Heyting algebras are distributive, that's true.
- All _arbitrary distributive_ lattices are equipped with Heyting operations, that's also true. 
- General distributive lattices don't promise arbitrary distributive law.

== TODO: Boolean algebras